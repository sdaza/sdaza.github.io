<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="Aoorj5Z96fS0WAJS5xLEvSb7Bn9yMznQrwPd3w8NErM"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Tools for power analysis with multiple comparisons | Sebastian Daza</title> <meta name="author" content="Sebastian Daza"> <meta name="description" content="Demography, Sociology, Simulation, Data Science"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%AF&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://sdaza.com/blog/2023/statistical-power/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sebastian </span>Daza</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">academia</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/teaching/">teaching</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/repos/">repos</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Tools for power analysis with multiple comparisons</h1> <p class="post-meta">September 26, 2023• Sebastian Daza</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/simulation"> <i class="fas fa-hashtag fa-sm"></i> simulation</a>   <a href="/blog/tag/python"> <i class="fas fa-hashtag fa-sm"></i> python</a>   <a href="/blog/tag/experiments"> <i class="fas fa-hashtag fa-sm"></i> experiments</a>   </p> </header> <article class="post-content"> <p>This post will discuss tools for designing and evaluating experiments. These tools offer user-friendly methods to estimate statistical power when working with multiple treatments or variants. They also help with sample allocation, including determining sample sizes for treatment and control groups. Lastly, they estimate power based on different expected effects (e.g., minimum detectable effects or MDE).</p> <p>Calculating power for complex and uplift models using machine learning can be challenging. I will provide some general guidelines and references at the end of this post.</p> <h2 id="lets-start-with-some-terms">Let’s start with some terms</h2> <p>When conducting a statistical test to estimate an effect, we can have any of the following outcomes:</p> <p align="center"> <img src="/assets/img/pregnant-power.jpg" alt="Image" width="70%" height="70%"> </p> <p>These errors and success probabilities are associated with standard statistical terms used in making inferences.</p> <ul> <li> <strong>Type I Error (\(\alpha\))</strong>: the probability of rejecting the null hypothesis when it’s true.</li> <li> <strong>Type II Error (\(\beta\))</strong>: the probability of failing to reject the null hypothesis when it’s false.</li> <li> <strong>Power (\(1-\beta\))</strong>: the probability of rejecting the null hypothesis when it’s false.</li> <li> <strong>Confidence (\(1-\alpha\))</strong>: the probability of failing to reject the null hypothesis when it’s true.</li> </ul> <p>As a standard, \(\alpha\) is set to 0.05 and power to .80. Also, we will generally have a control group (no intervention) and different treatments (variants). </p> <h2 id="there-are-some-challenges">There are some challenges</h2> <p>There are some challenges associated with basic power calculations (e.g., web calculators): </p> <ul> <li><strong>(Simultaneous) multiple comparisons</strong></li> <li><strong>Sample allocation</strong></li> <li><strong>Different MDEs by variant</strong></li> </ul> <p>When conducting multiple statistical tests or comparisons within a study or analysis, there is a risk of obtaining false positive results. To address this, we need to adjust our \(\alpha\) level based on the number of tests performed. This adjustment increases the required sample size to achieve statistical significance. It is important to consider this issue during experiment design and data analysis, including post hoc analyses.</p> <p>To gain an understanding of why multiple comparison testing poses a challenge, let’s consider a simple example. Suppose we have a control group and three treatment variants. In this example, we will simulate data where there are no effects or differences between the control and treatment groups. In other words, the null hypothesis (i.e., no effect) is always true. In each iteration, we test the control group against each treatment and count the number of significant results we obtain using a significance level (\(\alpha=0.05\)). It is important to note that we already know there are no differences between the groups.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">ttest_ind</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">pd</span><span class="p">.</span><span class="nf">set_option</span><span class="p">(</span><span class="sh">"</span><span class="s">display.notebook_repr_html</span><span class="sh">"</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Number of iterations for simulation
</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Number of treatment groups
</span><span class="n">groups</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Number of samples in each group
</span><span class="n">samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Count of batches where at least one false positive (Type I error) occurs
</span><span class="n">batches_with_errors</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># perform iterations
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="c1"># Control group data
</span>    <span class="n">control_group</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>

    <span class="c1"># Treatment group data - no actual difference from control
</span>    <span class="n">treatment_groups</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">groups</span><span class="p">)]</span>

    <span class="c1"># A flag for any error within this batch of tests
</span>    <span class="n">error_in_batch</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="c1"># perform t-test for each treatment group compared to control group
</span>    <span class="k">for</span> <span class="n">treatment</span> <span class="ow">in</span> <span class="n">treatment_groups</span><span class="p">:</span>
        <span class="n">t_stat</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="nf">ttest_ind</span><span class="p">(</span><span class="n">control_group</span><span class="p">,</span> <span class="n">treatment</span><span class="p">)</span>

        <span class="c1"># Check if p-value is less than 0.05 (significance level)
</span>        <span class="k">if</span> <span class="n">p_value</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
            <span class="n">error_in_batch</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">break</span>  <span class="c1"># no need to check remaining tests
</span>
    <span class="k">if</span> <span class="n">error_in_batch</span><span class="p">:</span>
        <span class="n">batches_with_errors</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Probability of at least one Type I error in a batch of </span><span class="si">{</span><span class="n">groups</span><span class="si">}</span><span class="s"> tests: </span><span class="si">{</span><span class="n">batches_with_errors</span> <span class="o">/</span> <span class="n">iterations</span> <span class="o">*</span> <span class="mi">100</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Probability of at least one Type I error in a batch of 3 tests: 12.6%
</code></pre></div></div> <p>After 1000 iterations, the probability of encountering at least one Type I error in a batch of 3 tests is 12.6%. You can also use the formula:</p> \[1 - (1 - \alpha)^n\] <p>Where \(n\) is the number of comparisons. For this example, FWER will be 14.2%. This probability is referred to as the <strong>Family-wise Error Rate (FWER)</strong>, which signifies the likelihood of committing at least one Type I error within the entire family (or batch) of comparisons. It is evident that 12.6% and 14.2% are higher than the assumed α value of 5% for a single test.</p> <p>This is the essence of the multiple comparison problem. When conducting multiple tests, even if you maintain an \(\alpha\) level of approximately 0.05 for each individual test, the likelihood of experiencing at least one false discovery among the entire set of tests is significantly higher and increases as the number of tests in the batch grows. Consequently, some of our significant findings may actually be false positives and difficult to replicate.</p> <p>The key to adjusting multiple comparison testing is to strike a balance between reducing false positives (incorrectly rejecting the null hypothesis) and maintaining test power (the ability to correctly reject the null hypothesis when it’s false). Some methods, such as <em>Bonferroni</em> or <em>Holm</em>, minimize the <strong>Family-Wise Error Rate (FWER)</strong>. Others, like <em>Benjamini-Hochberg</em>, minimize the <strong>False Discovery Rate (FDR)</strong>, which is the expected proportion of Type I errors among all declared significant hypotheses. FDR methods aim to reduce the ratio of Type I errors among significant results without eliminating all false discoveries like FWER methods. FDR methods generally offer higher statistical power and are less conservative.</p> <p>A crucial element of experimental design lies in sample allocation. Variations in the distribution of samples among groups can influence the statistical power of our tests. Sometimes, the goal is to limit the potential disruption from an intervention by scaling down the sample size across various variants. Alternatively, we might opt to shrink the control group’s size or allot more units to each variant, contingent on their Minimum Detectable Effect (MDE).</p> <p>Let’s not forget to consider expected effects. In figuring out the smallest minimum detectable effect (MDE), think about the least significant change that would still make the intervention worthwhile. It’s all about balancing the books: estimate the return on investment (ROI) from the intervention, then determine the tiniest shift in a key metric (like conversion rates) that would still make the effort profitable. The MDE doesn’t have to be identical across all variants - after all, not all interventions or campaigns cost the same.</p> <h2 id="tools-can-help">Tools can help</h2> <p>I’ve developed a set of straightforward simulation methods in Python, primarily to construct and evaluate our experimental data. The choice to use simulation was driven by its flexibility in handling diverse scenarios and metrics. However, it does come with a trade-off, as it demands a higher computational power. We can live with that <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">. <a href="https://github.com/sdaza/sdaza.github.io/tree/main/_jupyter/power_tools.py" rel="external nofollow noopener" target="_blank">Feel free to explore the code for the power class</a>.</p> <p>These methods are useful, particularly for handling multiple variants, different allocations, or MDE by group. However, it’s important to note that these methods only offer a power estimate based on a given set of parameters. Therefore, we will need to conduct a grid search to evaluate various scenarios and determine the optimal one.</p> <p>Let’s start with a simple example using a proportion as the metric of interest:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">power_tools</span> <span class="kn">import</span> <span class="o">*</span> 

<span class="c1"># load class
</span><span class="n">p</span> <span class="o">=</span> <span class="nc">PowerSim</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">proportion</span><span class="sh">'</span><span class="p">,</span> <span class="n">relative_effect</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">variants</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nsim</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="sh">'</span><span class="s">two-tailed</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># get power
</span><span class="n">p</span><span class="p">.</span><span class="nf">get_power</span><span class="p">(</span><span class="n">baseline</span><span class="o">=</span><span class="p">[</span><span class="mf">0.33</span><span class="p">],</span> <span class="n">effect</span><span class="o">=</span><span class="p">[</span><span class="mf">0.03</span><span class="p">],</span> <span class="n">sample_size</span><span class="o">=</span><span class="p">[</span><span class="mi">3000</span><span class="p">])</span></code></pre></figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  comparisons  power
0      (0, 1)  0.706
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">nsim</code> represents the number of simulations. Variants are set to 1, so we are only comparing control and treatment: <code class="language-plaintext highlighter-rouge">comparisons = (0,1)</code>. This example is too simple. Let’s now assume two variants (treatments): </p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">p</span> <span class="o">=</span> <span class="nc">PowerSim</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">proportion</span><span class="sh">'</span><span class="p">,</span> <span class="n">relative_effect</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">variants</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
             <span class="n">nsim</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="sh">'</span><span class="s">two-tailed</span><span class="sh">'</span><span class="p">)</span>
<span class="n">p</span><span class="p">.</span><span class="nf">get_power</span><span class="p">(</span><span class="n">baseline</span><span class="o">=</span><span class="p">[</span><span class="mf">0.33</span><span class="p">],</span> <span class="n">effect</span><span class="o">=</span><span class="p">[</span><span class="mf">0.03</span><span class="p">],</span> <span class="n">sample_size</span><span class="o">=</span><span class="p">[</span><span class="mi">3000</span><span class="p">])</span></code></pre></figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  comparisons  power
0      (0, 1)  0.531
1      (0, 2)  0.530
2      (1, 2)  0.015
</code></pre></div></div> <p>The function calculates the power of group comparisons with a sample size of 3000 for each group. Multiple comparisons result in reduced power. Since the effect of each variant is the same (0.03), comparing <em>variants 1 and 2</em> is not meaningful here (it will always be 0 in practice). Custom comparisons can be defined using a list of tuples, such as <code class="language-plaintext highlighter-rouge">comparisons=[(0,1), (0,2)]</code>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">p</span> <span class="o">=</span> <span class="nc">PowerSim</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">proportion</span><span class="sh">'</span><span class="p">,</span> <span class="n">relative_effect</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">variants</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
             <span class="n">nsim</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="sh">'</span><span class="s">two-tailed</span><span class="sh">'</span><span class="p">,</span>
             <span class="n">comparisons</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">p</span><span class="p">.</span><span class="nf">get_power</span><span class="p">(</span><span class="n">baseline</span><span class="o">=</span><span class="p">[</span><span class="mf">0.33</span><span class="p">],</span> <span class="n">effect</span><span class="o">=</span><span class="p">[</span><span class="mf">0.03</span><span class="p">],</span> <span class="n">sample_size</span><span class="o">=</span><span class="p">[</span><span class="mi">3000</span><span class="p">])</span></code></pre></figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  comparisons  power
0      (0, 1)  0.558
1      (0, 2)  0.563
</code></pre></div></div> <p>Using only two comparisons (each variant with a control), the power significantly decreases compared to the one-variant example. I have currently implemented the following p-value corrections:</p> <ul> <li><code class="language-plaintext highlighter-rouge">bonferroni</code></li> <li><code class="language-plaintext highlighter-rouge">holm_bonferroni</code></li> <li><code class="language-plaintext highlighter-rouge">hochberg</code></li> <li><code class="language-plaintext highlighter-rouge">sidak</code></li> <li><code class="language-plaintext highlighter-rouge">fdr</code></li> </ul> <p>You can find an introduction to different methods <a href="https://en.wikipedia.org/wiki/Multiple_comparisons_problem" rel="external nofollow noopener" target="_blank">here</a>. In general, Bonferroni is more conservative, while Holm provides higher power. The <code class="language-plaintext highlighter-rouge">fdr</code> method aims to minimize the false discovery rate.</p> <p>The function corrects only for the comparisons specified in the <code class="language-plaintext highlighter-rouge">comparisons</code> parameter. If you conduct additional tests, such as comparing the performance of group A versus group B or examining differences among demographic groups, you need to apply additional multiple comparison corrections in your analysis.</p> <p>We can introduce greater variability into the experimental design. We can then plot the expected power for different scenarios. To specify parameters, we’ll utilize nested lists and the <code class="language-plaintext highlighter-rouge">grid_sim_power</code> method. However, things can quickly become convoluted.</p> <p>Let’s consider a more complex example. The impact of the first and second variants on the control group varies as follows: <code class="language-plaintext highlighter-rouge">[0.01, 0.03]</code>, <code class="language-plaintext highlighter-rouge">[0.03, 0.05]</code>, <code class="language-plaintext highlighter-rouge">[0.03, 0.07]</code>. The sample sizes are equal for each group, but they increase linearly. We apply the <code class="language-plaintext highlighter-rouge">holm</code> correction.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">p</span> <span class="o">=</span> <span class="nc">PowerSim</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">proportion</span><span class="sh">'</span><span class="p">,</span> <span class="n">relative_effect</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">variants</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="sh">'</span><span class="s">two-tailed</span><span class="sh">'</span><span class="p">,</span> 
	<span class="n">nsim</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">correction</span><span class="o">=</span><span class="sh">'</span><span class="s">holm</span><span class="sh">'</span><span class="p">)</span>
<span class="n">rr</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">grid_sim_power</span><span class="p">(</span><span class="n">baseline_rates</span><span class="o">=</span><span class="p">[[</span><span class="mf">0.33</span><span class="p">]],</span> 
                <span class="n">effects</span><span class="o">=</span><span class="p">[[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">]],</span>
                <span class="n">sample_sizes</span><span class="o">=</span> <span class="p">[[</span><span class="mi">1000</span><span class="p">],</span> <span class="p">[</span><span class="mi">2000</span><span class="p">],</span> <span class="p">[</span><span class="mi">3000</span><span class="p">],</span> <span class="p">[</span><span class="mi">4000</span><span class="p">],</span> <span class="p">[</span><span class="mi">5000</span><span class="p">],</span> <span class="p">[</span><span class="mi">6000</span><span class="p">],</span> <span class="p">[</span><span class="mi">7000</span><span class="p">],</span> <span class="p">[</span><span class="mi">8000</span><span class="p">],</span> <span class="p">[</span><span class="mi">9000</span><span class="p">]],</span> 
                <span class="n">threads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> 
                <span class="n">plot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre></figure> <p><img src="/assets/img/2023-09-26-statistical-power_files/2023-09-26-statistical-power_10_0.png" alt="png"></p> <p><img src="/assets/img/2023-09-26-statistical-power_files/2023-09-26-statistical-power_10_1.png" alt="png"></p> <p><img src="/assets/img/2023-09-26-statistical-power_files/2023-09-26-statistical-power_10_2.png" alt="png"></p> <p>As expected, the best scenario occurs when the effect sizes are large enough to be detected. We can also analyze the results of different sample allocations. For example, when we look at the effects <code class="language-plaintext highlighter-rouge">[0.03, 0.05]</code>, the allocation <code class="language-plaintext highlighter-rouge">[3000, 7000, 7000]</code> produces fairly good results, although it doesn’t always meet the 0.8 threshold.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">p</span> <span class="o">=</span> <span class="nc">PowerSim</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">proportion</span><span class="sh">'</span><span class="p">,</span> <span class="n">relative_effect</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">variants</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
             <span class="n">alternative</span><span class="o">=</span><span class="sh">'</span><span class="s">two-tailed</span><span class="sh">'</span><span class="p">,</span> <span class="n">nsim</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">correction</span><span class="o">=</span><span class="sh">'</span><span class="s">holm</span><span class="sh">'</span><span class="p">)</span>
<span class="n">rr</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">grid_sim_power</span><span class="p">(</span><span class="n">baseline_rates</span><span class="o">=</span><span class="p">[[</span><span class="mf">0.33</span><span class="p">]],</span> 
                <span class="n">effects</span><span class="o">=</span><span class="p">[[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">]],</span>
                <span class="n">sample_sizes</span><span class="o">=</span> <span class="p">[[</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">],</span> 
                               <span class="p">[</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">],</span> 
                               <span class="p">[</span><span class="mi">3000</span><span class="p">,</span> <span class="mi">7000</span><span class="p">,</span> <span class="mi">7000</span><span class="p">],</span> 
                               <span class="p">[</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">8000</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
                               <span class="p">[</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">8000</span><span class="p">,</span> <span class="mi">8000</span><span class="p">]],</span>
                <span class="n">threads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> 
                <span class="n">plot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre></figure> <p><img src="/assets/img/2023-09-26-statistical-power_files/2023-09-26-statistical-power_12_0.png" alt="png"></p> <p><img src="/assets/img/2023-09-26-statistical-power_files/2023-09-26-statistical-power_12_1.png" alt="png"></p> <p><img src="/assets/img/2023-09-26-statistical-power_files/2023-09-26-statistical-power_12_2.png" alt="png"></p> <p>We can also use other metrics (e.g., average or counts).  For instance, we can design an experiment where the outcome is the number of visits (counts). The simulator will use a Poisson distribution with the parameter, \(\lambda\) (lambda) or the mean number of events. In this example, I set a baseline rate (lambda) of 1.2 visits and a relative increase of \(0.05 (1.2*1.05) = 1.26\), with a control group of 3000 users and a treatment of 5000 users:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">p</span> <span class="o">=</span> <span class="nc">PowerSim</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">count</span><span class="sh">'</span><span class="p">,</span> <span class="n">relative_effect</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">variants</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
             <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="sh">'</span><span class="s">two-tailed</span><span class="sh">'</span><span class="p">)</span>
<span class="n">p</span><span class="p">.</span><span class="nf">get_power</span><span class="p">(</span><span class="n">baseline</span><span class="o">=</span><span class="p">[</span><span class="mf">1.2</span><span class="p">],</span> <span class="n">effect</span><span class="o">=</span><span class="p">[</span><span class="mf">0.05</span><span class="p">],</span> <span class="n">sample_size</span><span class="o">=</span><span class="p">[</span><span class="mi">3000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">])</span></code></pre></figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  comparisons  power
0      (0, 1)   0.59
</code></pre></div></div> <p>The effect is small, so our test is underpowered (&lt;0.80). We can also use averages (e.g., revenue), but in that case, we need to specify the standard deviation of the groups:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">p</span> <span class="o">=</span> <span class="nc">PowerSim</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">average</span><span class="sh">'</span><span class="p">,</span> <span class="n">relative_effect</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">variants</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
             <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="sh">'</span><span class="s">two-tailed</span><span class="sh">'</span><span class="p">,</span> <span class="n">nsim</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">p</span><span class="p">.</span><span class="nf">get_power</span><span class="p">(</span><span class="n">baseline</span><span class="o">=</span><span class="p">[</span><span class="mi">1000</span><span class="p">],</span> <span class="n">effect</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">],</span> <span class="n">standard_deviation</span><span class="o">=</span><span class="p">[</span><span class="mi">600</span><span class="p">],</span> <span class="n">sample_size</span><span class="o">=</span><span class="p">[</span><span class="mi">400</span><span class="p">])</span></code></pre></figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  comparisons   power
0      (0, 1)  0.6592
</code></pre></div></div> <h2 id="complex-models">Complex models </h2> <p>When using uplift or mixed models, things become more complicated. As Aleksander Molak put it:</p> <blockquote> <p>The question of defining a “safe” dataset size for S-Learner and other causal models is difficult to answer. Power calculations for machine learning models are often difficult, if possible at all. </p> </blockquote> <p>There are some tricks we can apply, though, as Harrell suggests:</p> <blockquote> <p>If you can afford a pilot study or you have some historical data that represents a problem similar to the one that you’re interested in, you can find a subgroup in your data that is as homogenous as possible. You can estimate the sample size for this group using some of the traditional statistical power tools. Finally, scale your overall sample size so that this subgroup is properly powered relative to the entire sample</p> </blockquote> <p>There are also some traditional ways to optimize the power of our tests:</p> <ul> <li>Blocking design</li> <li>Stratification and covariate adjustments (regression)</li> </ul> <p>For instance, after learning from our uplift models, we can identify the key features associated to users’ responses. We can use those features to design our experiments (blocking). We can also evaluate the results of our uplift models in a new sample, and see if we can replicate the expected <code class="language-plaintext highlighter-rouge">uplift</code>.</p> <p>Here’s a brief and general overview of the key features to keep in mind when designing experiments or improving models. I hope it’s helpful.</p> <p><br></p> <hr> <h2 id="references">References</h2> <ul> <li>Molak, Aleksander. <em>Causal Inference and Discovery in Python: Unlock the secrets of modern causal machine learning with DoWhy, EconML, PyTorch and more</em> . </li> <li>https://www.fharrell.com/</li> <li>Ron Kohavi. <em>Trustworthy Online Controlled Experiments</em>.</li> </ul> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"sdaza/sdaza.github.io","data-repo-id":"MDEwOlJlcG9zaXRvcnk1MzY3MjE3MQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Sebastian Daza. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-4HQ7XRSJQG"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-4HQ7XRSJQG");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>